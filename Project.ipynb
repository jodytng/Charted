{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95124b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests #to load the page \n",
    "import csv\n",
    "import time\n",
    "from dateutil import parser\n",
    "from tqdm import tqdm #good to have\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver ## webdriver is the tool to interact with the webpage \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14e2b639",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (451893723.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [16], line 21\u001b[1;36m\u001b[0m\n\u001b[1;33m    if rating:\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def scrape_single_page(reviews, output, name):\n",
    "    for review in reviews: \n",
    "        date = review.find('span', attrs={'class':'css-chan6m'}).string\n",
    "        content = review.find('span', attrs={'class':'raw_09f24_T4Ezm'}).text.strip()\n",
    "        raw = review.find('span',attrs={'class':'raw_09f24_T4Ezm'}).get_text\n",
    "        temp = []\n",
    "        temp.append(name)\n",
    "        temp.append(date)\n",
    "        temp.append(content)\n",
    "        temp.append(raw)\n",
    "        votes = review.find_all('span', attrs={'class':'css-12i50in'})\n",
    "        for vote in votes:\n",
    "            count = vote.find('span', attrs={'class':'css-1lr1m88'})\n",
    "            if count: \n",
    "                temp.append(count.text)\n",
    "            else: \n",
    "                temp.append('0')\n",
    "                \n",
    "        ## fails if there are revised reviews\n",
    "        rating = review.find('div', attrs={'class':'star-rating'#change this accordingly})\n",
    "        if rating: \n",
    "            temp.append('5')\n",
    "        rating = review.find('div', attrs={'class':'star-rating'#change this accordingly})\n",
    "        if rating: \n",
    "            temp.append('4')\n",
    "        rating = review.find('div', attrs={'class':'star-rating'#change this accordingly})\n",
    "        if rating: \n",
    "            temp.append('3')\n",
    "        rating = review.find('div', attrs={'class':'star-rating'#change this accordingly})\n",
    "        if rating: \n",
    "            temp.append('2')\n",
    "        rating = review.find('div', attrs={'class':'star-rating'#change this accordingly})\n",
    "        if rating: \n",
    "            temp.append('1')\n",
    "        output.append(temp)\n",
    "\n",
    "header = ['name','date','content','raw','useful','funny','cool','rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f963d67",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'restaurant_list.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/_f/3fs6vlxj07v9cgw4s2z4b6d40000gn/T/ipykernel_21012/1360283961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'restaurant_list.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf_8_sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcsvreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrestaurant_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsvreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'restaurant_list.csv'"
     ]
    }
   ],
   "source": [
    "file = open('restaurant_list.csv', encoding = 'utf_8_sig')\n",
    "csvreader = csv.reader(file)\n",
    "restaurant_list = []\n",
    "next(csvreader) \n",
    "for row in csvreader: \n",
    "    url = row[1]\n",
    "    restaurant_list.append(url)\n",
    "file.close()\n",
    "\n",
    "for url in restaurant_list: \n",
    "    output = []\n",
    "    response = requests.get(url)\n",
    "    page = BeautifulSoup(response.text, 'html.parser')  \n",
    "    \n",
    "    #scrape the name of the restaurant \n",
    "    name = page.find('h1', attrs = {'class': 'css-12dgwvn'}).text #inspect html \n",
    "    reviews = page.find_all('div', attrs = {'class': 'css-12dgwvn'}) #inspect html \n",
    "    scrape_single_page(reviews, output, name)\n",
    "    \n",
    "    num_reviews = page.find('span', attrs = {'class': 'css-12dgwvn'}).string\n",
    "    num_reviews = int(num_reviews.split('')[0])\n",
    "    page_section = page.find('div', attrs = {'class': 'css-12dgwvn'})\n",
    "    num_pages = page_section.find('span', attrs = {'class': 'css-12dgwvn'}).text\n",
    "    num_pages = int(num_pages.split('')[-1]) - 1 \n",
    "    \n",
    "    url_list = []\n",
    "    for i in range(10, num_pages*10, 10):\n",
    "        url_list.append(url + '?start=' + str(i))\n",
    "        \n",
    "        \n",
    "    print(name + ' ; ' + str(num_pages) + ' ; ' + strlen((url_list)) + ' ; ' + str(num_reviews))\n",
    "    time.sleep(1)\n",
    "    \n",
    "    for new_url in tqdm(url_list): \n",
    "        num_reviews = 0\n",
    "        sleep_time = 10\n",
    "        sleep_num = 0\n",
    "        \n",
    "        while num_reviews == 0: \n",
    "            time.sleep(sleep_time)\n",
    "            sleep_num += 1\n",
    "            response = requests.get(new_url)\n",
    "            page = BeautifulSoup(response.text, 'html_parser')\n",
    "            reviews = page.find_all('div', attrs = {'class': '.....'}) #inspect html\n",
    "            num_reviews = len(reviews)\n",
    "            if sleep_num > 2: \n",
    "                print(\"sleep more than 3 for: \" + new_url)\n",
    "        scrape_single_page(reviews, output, name)\n",
    "        \n",
    "    cur_size = len(output)\n",
    "    print('newly added: ' + str(cur_size))\n",
    "    \n",
    "    file name = name + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df914181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spotify imports\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "from requests import post, get\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c1d01d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing API calls using Spotify tokens'\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client_id = os.getenv(\"CLIENT_ID\")\n",
    "client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "\n",
    "def get_token():\n",
    "    auth_string = client_id + \":\" + client_secret\n",
    "    auth_bytes = auth_string.encode(\"utf-8\")\n",
    "    auth_base64 = str(base64.b64encode(auth_bytes), \"utf-8\")\n",
    "\n",
    "    url = \"https://accounts.spotify.com/api/token\"\n",
    "    headers = {\n",
    "        \"Authorization\": \"Basic \" + auth_base64,\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "    data = {\"grant_type\": \"client_credentials\"}\n",
    "    result = post(url, headers = headers, data = data)\n",
    "    json_result = json.loads(result.content)\n",
    "    token = json_result['access_token']\n",
    "    return token \n",
    "\n",
    "def get_auth_header(token):\n",
    "    return {\"Authorization\": \"Bearer \" + token}\n",
    "\n",
    "\n",
    "# here we have obtained the token we can use to get the attributes we want from our songs\n",
    "token = get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'danceability': 0.735, 'energy': 0.478, 'key': 2, 'loudness': -12.472, 'mode': 1, 'speechiness': 0.0293, 'acousticness': 0.189, 'instrumentalness': 2.48e-06, 'liveness': 0.355, 'valence': 0.947, 'tempo': 107.682, 'type': 'audio_features', 'id': '2FRnf9qhLbvw8fu4IBXx78', 'uri': 'spotify:track:2FRnf9qhLbvw8fu4IBXx78', 'track_href': 'https://api.spotify.com/v1/tracks/2FRnf9qhLbvw8fu4IBXx78', 'analysis_url': 'https://api.spotify.com/v1/audio-analysis/2FRnf9qhLbvw8fu4IBXx78', 'duration_ms': 262960, 'time_signature': 4}\n"
     ]
    }
   ],
   "source": [
    "#search for the track on spotify api\n",
    "def search_for_track(token, track_name):\n",
    "    url = \"https://api.spotify.com/v1/search\"\n",
    "    headers = get_auth_header(token)\n",
    "    query = f\"?q={track_name}&type=track&limit=1\"\n",
    "\n",
    "    query_url = url + query\n",
    "    result = get(query_url, headers = headers)\n",
    "    json_result = json.loads(result.content)\n",
    "    if len(json_result) == 0:\n",
    "        print(\"song not found\")\n",
    "        return None\n",
    "    return json_result\n",
    "\n",
    "#slice the track call to get track id\n",
    "def get_track_id(track_dict):\n",
    "    return track_dict['tracks']['items'][0]['id']\n",
    "\n",
    "#extract track features \n",
    "def get_track_features(token, track_id):\n",
    "    url = f\"https://api.spotify.com/v1/audio-features/{track_id}?country=US\"\n",
    "    headers = get_auth_header(token)\n",
    "    #query = f\"?{track_id}\"\n",
    "\n",
    "    query_url = url #+ query\n",
    "    result = get(query_url, headers = headers)\n",
    "    json_result = json.loads(result.content)\n",
    "    return json_result\n",
    "\n",
    "#print(search_for_track(token,\"ghost\")['tracks']['items'][0]['id'])\n",
    "#print(type(get_track_features(token, \"6I3mqTwhRpn34SLVafSH7G\")))\n",
    "\n",
    "track = search_for_track(token, 'Last Christmas')\n",
    "track_id = get_track_id(track)\n",
    "features = get_track_features(token, track_id)\n",
    "\n",
    "print(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e052c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22bbc23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
